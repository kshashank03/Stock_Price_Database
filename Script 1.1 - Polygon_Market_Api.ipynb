{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "import os\n",
    "import math\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some constant variables, I could put all of this in a seperate config file\n",
    "ALPACA_API_KEY = 'AKBUXI6D3LXKNW1FICFH'\n",
    "START_DATE = '2005-01-03'\n",
    "END_DATE = '2020-12-31'\n",
    "# URL for all the tickers on Polygon\n",
    "POLYGON_TICKERS_URL = 'https://api.polygon.io/v2/reference/tickers?page={}&apiKey={}'\n",
    "# URL FOR PRICING DATA - Note, getting pricing that is UNADJUSTED for splits, I will try and adjust those manually\n",
    "POLYGON_AGGS_URL = 'https://api.polygon.io/v2/aggs/ticker/{}/range/1/day/{}/{}?unadjusted=true&apiKey={}'\n",
    "# URL FOR DIVIDEND DATA\n",
    "POLYGON_DIV_URL = 'https://api.polygon.io/v2/reference/dividends/{}?apiKey={}'\n",
    "# URL FOR STOCK SPLITS\n",
    "POLYGON_SPLIT_URL = 'https://api.polygon.io/v2/reference/splits/{}?apiKey={}'\n",
    "#URL FOR TICKER TYPES\n",
    "POLYGON_TYPES_URL = 'https://api.polygon.io/v2/reference/types?apiKey={}'\n",
    "#URL FOR TICKER Info\n",
    "POLYGON_TICKER_INFO_URL = \"https://api.polygon.io/v1/meta/symbols/{}/company?apiKey={}\""
   ]
  },
  {
   "source": [
    "## Ticker Info API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"logo\":\"https://s3.polygon.io/logos/aapl/logo.png\",\"listdate\":\"1990-01-02\",\"cik\":\"320193\",\"bloomberg\":\"EQ0010169500001000\",\"figi\":null,\"lei\":\"HWUPKR0MPOU8FGXBT394\",\"sic\":3571,\"country\":\"usa\",\"industry\":\"Computer Hardware\",\"sector\":\"Technology\",\"marketcap\":908316631180,\"employees\":123000,\"phone\":\"+1 408 996-1010\",\"ceo\":\"Timothy D. Cook\",\"url\":\"http://www.apple.com\",\"description\":\"Apple Inc is designs, manufactures and markets mobile communication and media devices and personal computers, and sells a variety of related software, services, accessories, networking solutions and third-party digital content and applications.\",\"exchange\":\"Nasdaq Global Select\",\"name\":\"Apple Inc.\",\"symbol\":\"AAPL\",\"exchangeSymbol\":\"NGS\",\"hq_address\":\"1 Infinite Loop Cupertino CA, 95014\",\"hq_state\":\"CA\",\"hq_country\":\"USA\",\"type\":\"CS\",\"updated\":\"11/16/2018\",\"tags\":[\"Technology\",\"Consumer Electronics\",\"Computer Hardware\"],\"similar\":[\"MSFT\",\"NOK\",\"IBM\",\"HPQ\",\"GOOGL\",\"BB\",\"XLK\"],\"active\":true}'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "requests.get(POLYGON_TICKER_INFO_URL.format(\"AAPL\", ALPACA_API_KEY)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all supported tickers from Polygon.io\n",
    "def get_tickers(url = POLYGON_TICKERS_URL):\n",
    "    page = 1\n",
    "\n",
    "    session = requests.Session()\n",
    "    # Initial request to get the ticker count\n",
    "    r = session.get(POLYGON_TICKERS_URL.format(page, ALPACA_API_KEY))\n",
    "    data = r.json()\n",
    "\n",
    "    # This is to figure out how many pages to run pagination \n",
    "    count = data['count']\n",
    "    print('total tickers ' + str(count))\n",
    "    pages = math.ceil(count / data['perPage'])\n",
    "\n",
    "    # Pull in all the pages of tickers\n",
    "    for pages in range (2, pages+1):  # For production\n",
    "    # for pages in range (2, 10):  # For testing\n",
    "        r = session.get(POLYGON_TICKERS_URL.format(page, ALPACA_API_KEY))\n",
    "        data = r.json()\n",
    "        df = pd.DataFrame(data['tickers'])\n",
    "        df.to_csv('data/tickers/{}.csv'.format(page), index=False)\n",
    "        print('Page {} processed'.format(page))\n",
    "        page += 1\n",
    "        \n",
    "    return('Processes {} pages of tickers'.format(page-1))\n",
    "\n",
    "\n",
    "# Stich all of these csv files into one dataframe for analysis\n",
    "def combine_tickers(directory):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for f in os.listdir(directory):\n",
    "        df2 = pd.read_csv('{}/{}'.format(directory, f))\n",
    "        df = df.append(df2)\n",
    "    \n",
    "    # Read out a copy of the file to a csv for later analysis\n",
    "    df.set_index('ticker', inplace=True)\n",
    "    df.drop_duplicates()  # Just in case any tickers get pulled twice\n",
    "    df.to_csv('polygon_tickers.csv')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_us_exch(ticker_df):\n",
    "    \n",
    "    # Keep only U.S. Dollar denominated securities\n",
    "    df = ticker_df[(ticker_df.currency == 'USD') & (ticker_df.locale == 'US')]\n",
    "    # Keep only the primary U.S. exchanges\n",
    "    exch = ['AMX','ARCA','BATS','NASDAQ','NSC','NYE']\n",
    "    df = df[df['primaryExch'].isin(exch)]\n",
    "    # Filter out preferred stock, american depositry receipts, closed end funds, reit\n",
    "    stockTypes = ['PFD','ADR','CEF','MLP','REIT','RIGHT','UNIT','WRT']\n",
    "    df = df[df['type'].isin(stockTypes) == False]\n",
    "    \n",
    "    df.to_csv('polygon_tickers_us.csv')\n",
    "\n",
    "    # Create a list of symbols to loop thru\n",
    "    symbols = df.index.tolist()\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "# Get the aggregated bars for the symbols I need\n",
    "def get_bars(symbolslist, outdir, start, end):\n",
    "\n",
    "    session = requests.Session()\n",
    "    # In case I run into issues, retry my connection\n",
    "    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    count = 0\n",
    "    \n",
    "    barlog = open(\"barlog.txt\", \"w\")\n",
    "    \n",
    "    for symbol in symbolslist:\n",
    "        try:\n",
    "            r = session.get(POLYGON_AGGS_URL.format(symbol, start, end, ALPACA_API_KEY))\n",
    "            if r:\n",
    "                data = r.json()\n",
    "            \n",
    "                # create a pandas dataframe from the information\n",
    "                if data['queryCount'] > 1:\n",
    "                    df = pd.DataFrame(data['results'])\n",
    "                    df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "                    df['date'] =  df['date'].dt.date.astype(str)\n",
    "                    df.set_index('date', inplace=True)\n",
    "                    df['symbol'] = symbol\n",
    "\n",
    "                    df.drop(columns=['vw', 't', 'n'], inplace=True)\n",
    "                    df.rename(columns={'v': 'volume', 'o': 'open', 'c': 'close', 'h': 'high', 'l': 'low'}, inplace=True)\n",
    "\n",
    "                    df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "                    count += 1\n",
    "\n",
    "                    # Logging, I could write a short method for this to reuse\n",
    "                    msg = (symbol + ' file created with record count ' + str(data['queryCount']))\n",
    "                    print(msg)\n",
    "                    barlog.write(msg)\n",
    "                    barlog.write(\"\\n\")\n",
    "\n",
    "                else:\n",
    "                    msg = ('No data for symbol ' + str(symbol))\n",
    "                    print(msg)\n",
    "                    barlog.write(msg)\n",
    "                    barlog.write(\"\\n\")\n",
    "            else:\n",
    "                msg = ('No response for symbol ' + str(symbol))\n",
    "                print(msg)\n",
    "                barlog.write(msg)\n",
    "                barlog.write(\"\\n\")\n",
    "        # Raise exception but continue           \n",
    "        except:\n",
    "            msg = ('****** exception raised for symbol ' + str(symbol))\n",
    "            print(msg)\n",
    "            barlog.write(msg)\n",
    "            barlog.write(\"\\n\")\n",
    "    \n",
    "    barlog.close()\n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Define a function to pull in the splits data\n",
    "def get_splits(symbolslist, outdir):\n",
    "\n",
    "    session = requests.Session()\n",
    "    # In case I run into issues, retry my connection\n",
    "    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    count = 0\n",
    "    \n",
    "    # Get the split data\n",
    "    for symbol in symbolslist:\n",
    "        try:\n",
    "            r = session.get(POLYGON_SPLIT_URL.format(symbol, ALPACA_API_KEY))\n",
    "            if r:\n",
    "                data = r.json()\n",
    "                if data['count'] > 0:\n",
    "                    df = pd.DataFrame(data['results'])\n",
    "                    df.rename(columns={'exDate': 'date', 'declaredDate': 'splitDeclaredDate'}, inplace=True)\n",
    "                    df.drop(columns=['paymentDate'], inplace=True)\n",
    "                    df.set_index('date', inplace=True)\n",
    "                    df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "                    \n",
    "                    print('split file for ' + symbol + ' ' + str(data['count']))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print('No data for symbol ' + str(symbol))\n",
    "            else:\n",
    "                print('No response for symbol ' + str(symbol))\n",
    "        # Raise exception but continue           \n",
    "        except:\n",
    "            print('****** exception raised for symbol ' + str(symbol))\n",
    "            \n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Fix erroneous splits from a correction file manually created\n",
    "def fix_splits(splitpath):\n",
    "    # Get the split corrections to overwrite\n",
    "    correct_df = pd.read_csv('split_corrections.csv')\n",
    "    # create a list of symbols to fix\n",
    "    symbols = correct_df['ticker'].tolist()\n",
    "    # remove duplicates\n",
    "    symbols = list(dict.fromkeys(symbols))\n",
    "\n",
    "    # for symbol in symbols:\n",
    "    for symbol in symbols:\n",
    "        print(symbol)\n",
    "\n",
    "    # get any splits\n",
    "        if os.path.isfile('{}/{}.csv'.format(splitpath, symbol)):\n",
    "            df = pd.read_csv('{}/{}.csv'.format(splitpath, symbol))\n",
    "            print(df)\n",
    "            df = pd.merge(df, correct_df, how='left', left_on=['date', 'ticker'], right_on=['date', 'ticker'])\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                # Adjust bad dates\n",
    "                if not pd.isnull(row.date_adj):\n",
    "                    df.loc[index, 'date'] = row.date_adj\n",
    "                # Adjust bad ratios\n",
    "                if not pd.isnull(row.ratio_adj):\n",
    "                    df.loc[index, 'ratio'] = row.ratio_adj\n",
    "                else:\n",
    "                    df.loc[index, 'ratio'] = row.ratio_x\n",
    "            \n",
    "            # Format the dataframe for export\n",
    "            df = df[['date', 'ticker', 'ratio']]\n",
    "            df.set_index('date', inplace=True)\n",
    "            print(df)\n",
    "\n",
    "            # Overwrite the file with this new file\n",
    "            df.to_csv('{}/{}.csv'.format(splitpath, symbol))\n",
    "            print('Split file for {} corrected'.format(symbol))\n",
    "            \n",
    "        else:\n",
    "            print('no file found')\n",
    "                \n",
    "    return ('Split file corrections complete')\n",
    "\n",
    "\n",
    "# Define a function to pull in the splits data\n",
    "def get_divs(symbolslist, outdir):\n",
    "\n",
    "    session = requests.Session()\n",
    "    count = 0\n",
    "    \n",
    "    # Get the split data\n",
    "    for symbol in symbolslist: # ['AAPL']:\n",
    "        r = session.get(POLYGON_DIV_URL.format(symbol, ALPACA_API_KEY))\n",
    "        data = r.json()\n",
    "        if data['count'] > 0:\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            # df.rename(columns={'paymentDate': 'date'}, inplace=True)\n",
    "            df.rename(columns={'exDate': 'date', 'amount': 'dividend',\n",
    "                               'paymentDate': 'divPaymentDate',\n",
    "                               'recordDate': 'divRecordDate',\n",
    "                               'declaredDate': 'divDeclaredDate'}, inplace=True)\n",
    "            df.set_index('date', inplace=True)\n",
    "            df = df.groupby(df.index).first()\n",
    "            df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "            \n",
    "            print('div file for ' + symbol + ' ' + str(data['count']))\n",
    "            count += 1\n",
    "            \n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Combine bars, splits and dividend\n",
    "def combine_bars(barpath, splitpath, divpath):\n",
    "\n",
    "    count = 0\n",
    "    for f in os.listdir(barpath):\n",
    "        \n",
    "        symbol = f[:-4]\n",
    "        print(symbol)\n",
    "        \n",
    "        # Get the bar data\n",
    "        if os.path.isfile('{}/{}.csv'.format(barpath, symbol)):\n",
    "            bars = pd.read_csv('{}/{}.csv'.format(barpath, symbol), index_col='date')\n",
    "            \n",
    "            # get any splits\n",
    "            if os.path.isfile('{}/{}.csv'.format(splitpath, symbol)):\n",
    "                splits = pd.read_csv('{}/{}.csv'.format(splitpath, symbol), index_col='date')\n",
    "                splits.drop(columns=['ticker'], inplace=True)\n",
    "                \n",
    "                bars = bars.merge(splits, left_index=True, right_index=True, how='left')\n",
    "\n",
    "            else:\n",
    "                \n",
    "                bars = bars\n",
    "            \n",
    "            # get any dividend payments\n",
    "            if os.path.isfile('{}/{}.csv'.format(divpath, symbol)):\n",
    "                divs = pd.read_csv('{}/{}.csv'.format(divpath, symbol), index_col='date')\n",
    "                divs.drop(columns=['ticker'], inplace=True)\n",
    "            \n",
    "                bars = bars.merge(divs, left_index=True, right_index=True, how='left')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                bars = bars\n",
    "                \n",
    "            # Export bars \n",
    "            bars.to_csv('data/bars_adj/{}.csv'.format(symbol))\n",
    "            count += 1\n",
    "        \n",
    "    return ('{} adjusted bar file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Adjust the OHLCV data for stock splits\n",
    "def adj_bars(directory):\n",
    "\n",
    "    count = 0\n",
    "    for f in os.listdir(directory):\n",
    "\n",
    "        df = pd.read_csv('{}/{}'.format(directory, f), index_col='date')\n",
    "        \n",
    "        if 'ratio' in df.columns:\n",
    "            df['ratio_adj'] = df['ratio']\n",
    "        else:\n",
    "             df['ratio_adj'] = 1\n",
    "\n",
    "        # Create a split factor, shifted to the day earlier.  Also, fill in any missing factors with 1\n",
    "        df['split_factor'] = (1 / df['ratio_adj'].shift(-1)).fillna(1)\n",
    "        #  Create a cumulative product of the splits, in reverse order using the []::-1]\n",
    "        df['split_factor'] = df['split_factor'][::-1].cumprod()\n",
    "\n",
    "        # Adjust the various OHLCV metrics\n",
    "        df['volume_adj'] = df['volume'] * df['split_factor']\n",
    "        df['open_adj'] = df['open'] / df['split_factor']\n",
    "        df['close_adj'] = df['close'] / df['split_factor']\n",
    "        df['high_adj'] = df['high'] / df['split_factor']\n",
    "        df['low_adj'] = df['low'] / df['split_factor']\n",
    "        df['dollar_volume'] = df['volume'] * df['close']\n",
    "\n",
    "        df.to_csv('{}/{}'.format(directory, f))\n",
    "        count += 1\n",
    "        \n",
    "    return ('{} files was adjusted'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total tickers 122669\n",
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "Page 31 processed\n",
      "Page 32 processed\n",
      "Page 33 processed\n",
      "Page 34 processed\n",
      "Page 35 processed\n",
      "Page 36 processed\n",
      "Page 37 processed\n",
      "Page 38 processed\n",
      "Page 39 processed\n",
      "Page 40 processed\n",
      "Page 41 processed\n",
      "Page 42 processed\n",
      "Page 43 processed\n",
      "Page 44 processed\n",
      "Page 45 processed\n",
      "Page 46 processed\n",
      "Page 47 processed\n",
      "Page 48 processed\n",
      "Page 49 processed\n",
      "Page 50 processed\n",
      "Page 51 processed\n",
      "Page 52 processed\n",
      "Page 53 processed\n",
      "Page 54 processed\n",
      "Page 55 processed\n",
      "Page 56 processed\n",
      "Page 57 processed\n",
      "Page 58 processed\n",
      "Page 59 processed\n",
      "Page 60 processed\n",
      "Page 61 processed\n",
      "Page 62 processed\n",
      "Page 63 processed\n",
      "Page 64 processed\n",
      "Page 65 processed\n",
      "Page 66 processed\n",
      "Page 67 processed\n",
      "Page 68 processed\n",
      "Page 69 processed\n",
      "Page 70 processed\n",
      "Page 71 processed\n",
      "Page 72 processed\n",
      "Page 73 processed\n",
      "Page 74 processed\n",
      "Page 75 processed\n",
      "Page 76 processed\n",
      "Page 77 processed\n",
      "Page 78 processed\n",
      "Page 79 processed\n",
      "Page 80 processed\n",
      "Page 81 processed\n",
      "Page 82 processed\n",
      "Page 83 processed\n",
      "Page 84 processed\n",
      "Page 85 processed\n",
      "Page 86 processed\n",
      "Page 87 processed\n",
      "Page 88 processed\n",
      "Page 89 processed\n",
      "Page 90 processed\n",
      "Page 91 processed\n",
      "Page 92 processed\n",
      "Page 93 processed\n",
      "Page 94 processed\n",
      "Page 95 processed\n",
      "Page 96 processed\n",
      "Page 97 processed\n",
      "Page 98 processed\n",
      "Page 99 processed\n",
      "Page 100 processed\n",
      "Page 101 processed\n",
      "Page 102 processed\n",
      "Page 103 processed\n",
      "Page 104 processed\n",
      "Page 105 processed\n",
      "Page 106 processed\n",
      "Page 107 processed\n",
      "Page 108 processed\n",
      "Page 109 processed\n",
      "Page 110 processed\n",
      "Page 111 processed\n",
      "Page 112 processed\n",
      "Page 113 processed\n",
      "Page 114 processed\n",
      "Page 115 processed\n",
      "Page 116 processed\n",
      "Page 117 processed\n",
      "Page 118 processed\n",
      "Page 119 processed\n",
      "Page 120 processed\n",
      "Page 121 processed\n",
      "Page 122 processed\n",
      "Page 123 processed\n",
      "Page 124 processed\n",
      "Page 125 processed\n",
      "Page 126 processed\n",
      "Page 127 processed\n",
      "Page 128 processed\n",
      "Page 129 processed\n",
      "Page 130 processed\n",
      "Page 131 processed\n",
      "Page 132 processed\n",
      "Page 133 processed\n",
      "Page 134 processed\n",
      "Page 135 processed\n",
      "Page 136 processed\n",
      "Page 137 processed\n",
      "Page 138 processed\n",
      "Page 139 processed\n",
      "Page 140 processed\n",
      "Page 141 processed\n",
      "Page 142 processed\n",
      "Page 143 processed\n",
      "Page 144 processed\n",
      "Page 145 processed\n",
      "Page 146 processed\n",
      "Page 147 processed\n",
      "Page 148 processed\n",
      "Page 149 processed\n",
      "Page 150 processed\n",
      "Page 151 processed\n",
      "Page 152 processed\n",
      "Page 153 processed\n",
      "Page 154 processed\n",
      "Page 155 processed\n",
      "Page 156 processed\n",
      "Page 157 processed\n",
      "Page 158 processed\n",
      "Page 159 processed\n",
      "Page 160 processed\n",
      "Page 161 processed\n",
      "Page 162 processed\n",
      "Page 163 processed\n",
      "Page 164 processed\n",
      "Page 165 processed\n",
      "Page 166 processed\n",
      "Page 167 processed\n",
      "Page 168 processed\n",
      "Page 169 processed\n",
      "Page 170 processed\n",
      "Page 171 processed\n",
      "Page 172 processed\n",
      "Page 173 processed\n",
      "Page 174 processed\n",
      "Page 175 processed\n",
      "Page 176 processed\n",
      "Page 177 processed\n",
      "Page 178 processed\n",
      "Page 179 processed\n",
      "Page 180 processed\n",
      "Page 181 processed\n",
      "Page 182 processed\n",
      "Page 183 processed\n",
      "Page 184 processed\n",
      "Page 185 processed\n",
      "Page 186 processed\n",
      "Page 187 processed\n",
      "Page 188 processed\n",
      "Page 189 processed\n",
      "Page 190 processed\n",
      "Page 191 processed\n",
      "Page 192 processed\n",
      "Page 193 processed\n",
      "Page 194 processed\n",
      "Page 195 processed\n",
      "Page 196 processed\n",
      "Page 197 processed\n",
      "Page 198 processed\n",
      "Page 199 processed\n",
      "Page 200 processed\n",
      "Page 201 processed\n",
      "Page 202 processed\n",
      "Page 203 processed\n",
      "Page 204 processed\n",
      "Page 205 processed\n",
      "Page 206 processed\n",
      "Page 207 processed\n",
      "Page 208 processed\n",
      "Page 209 processed\n",
      "Page 210 processed\n",
      "Page 211 processed\n",
      "Page 212 processed\n",
      "Page 213 processed\n",
      "Page 214 processed\n",
      "Page 215 processed\n",
      "Page 216 processed\n",
      "Page 217 processed\n",
      "Page 218 processed\n",
      "Page 219 processed\n",
      "Page 220 processed\n",
      "Page 221 processed\n",
      "Page 222 processed\n",
      "Page 223 processed\n",
      "Page 224 processed\n",
      "Page 225 processed\n",
      "Page 226 processed\n",
      "Page 227 processed\n",
      "Page 228 processed\n",
      "Page 229 processed\n",
      "Page 230 processed\n"
     ]
    }
   ],
   "source": [
    "# #%%  Get all the tickers on Polygon.io and save them to a data directory\n",
    "# get_tickers()\n",
    "\n",
    "# #%% Combine all the paginated ticker files together into one dataframe\n",
    "# symbols = combine_tickers('data/tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  Name       Sector Symbol\n",
       "0           3M Company  Industrials    MMM\n",
       "1      A.O. Smith Corp  Industrials    AOS\n",
       "2  Abbott Laboratories  Health Care    ABT"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Sector</th>\n      <th>Symbol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3M Company</td>\n      <td>Industrials</td>\n      <td>MMM</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A.O. Smith Corp</td>\n      <td>Industrials</td>\n      <td>AOS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abbott Laboratories</td>\n      <td>Health Care</td>\n      <td>ABT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "sp_constituents = pd.read_json(\"https://datahub.io/core/s-and-p-500-companies/r/constituents.json\")\n",
    "sp_constituents.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " created with record count 1042\n",
      "LVS file created with record count 4028\n",
      "LEG file created with record count 4028\n",
      "LDOS file created with record count 1828\n",
      "LEN file created with record count 4028\n",
      "LLY file created with record count 4028\n",
      "LNC file created with record count 4028\n",
      "LIN file created with record count 1180\n",
      "LYV file created with record count 3782\n",
      "LKQ file created with record count 2168\n",
      "LMT file created with record count 4028\n",
      "L file created with record count 3495\n",
      "LOW file created with record count 4028\n",
      "LYB file created with record count 2572\n",
      "MTB file created with record count 4028\n",
      "MRO file created with record count 4028\n",
      "MPC file created with record count 2393\n",
      "MKTX file created with record count 4028\n",
      "MAR file created with record count 4028\n",
      "MMC file created with record count 4028\n",
      "MLM file created with record count 4028\n",
      "MAS file created with record count 4028\n",
      "MA file created with record count 3677\n",
      "MXIM file created with record count 3771\n",
      "MKC file created with record count 4028\n",
      "MCD file created with record count 4028\n",
      "MCK file created with record count 4028\n",
      "MDT file created with record count 4028\n",
      "MRK file created with record count 4028\n",
      "MET file created with record count 4028\n",
      "MTD file created with record count 4028\n",
      "MGM file created with record count 4013\n",
      "MCHP file created with record count 4028\n",
      "MU file created with record count 4028\n",
      "MSFT file created with record count 4028\n",
      "MAA file created with record count 4028\n",
      "MHK file created with record count 4028\n",
      "TAP file created with record count 4002\n",
      "MDLZ file created with record count 2076\n",
      "MNST file created with record count 3232\n",
      "MCO file created with record count 4028\n",
      "MS file created with record count 4010\n",
      "MSI file created with record count 3268\n",
      "MSCI file created with record count 2643\n",
      "MYL file created with record count 3997\n",
      "NDAQ file created with record count 4001\n",
      "NOV file created with record count 3980\n",
      "NTAP file created with record count 4028\n",
      "NFLX file created with record count 4028\n",
      "NWL file created with record count 4028\n",
      "NEM file created with record count 4028\n",
      "NWSA file created with record count 3024\n",
      "NWS file created with record count 4028\n",
      "NEE file created with record count 2651\n",
      "NLSN file created with record count 2501\n",
      "NKE file created with record count 4028\n",
      "NI file created with record count 4028\n",
      "NBL file created with record count 3966\n",
      "NSC file created with record count 4028\n",
      "NTRS file created with record count 4028\n",
      "NOC file created with record count 4028\n",
      "NLOK file created with record count 292\n",
      "NCLH file created with record count 2003\n",
      "NRG file created with record count 4028\n",
      "NUE file created with record count 4028\n",
      "NVDA file created with record count 4028\n",
      "NVR file created with record count 4028\n",
      "ORLY file created with record count 4028\n",
      "OXY file created with record count 4028\n",
      "ODFL file created with record count 4028\n",
      "OMC file created with record count 4028\n",
      "OKE file created with record count 4028\n",
      "ORCL file created with record count 4028\n",
      "OTIS file created with record count 189\n",
      "PCAR file created with record count 4028\n",
      "PKG file created with record count 4028\n",
      "PH file created with record count 4028\n",
      "PAYX file created with record count 4028\n",
      "PAYC file created with record count 1692\n",
      "PYPL file created with record count 1375\n",
      "PNR file created with record count 4028\n",
      "PBCT file created with record count 4008\n",
      "PEP file created with record count 4028\n",
      "PKI file created with record count 4028\n",
      "PRGO file created with record count 4028\n",
      "PFE file created with record count 4028\n",
      "PM file created with record count 3214\n",
      "PSX file created with record count 2183\n",
      "PNW file created with record count 4028\n",
      "PXD file created with record count 4028\n",
      "PNC file created with record count 4028\n",
      "PPG file created with record count 4028\n",
      "PPL file created with record count 4028\n",
      "PFG file created with record count 4028\n",
      "PG file created with record count 4028\n",
      "PGR file created with record count 4028\n",
      "PLD file created with record count 4028\n",
      "PRU file created with record count 4028\n",
      "PEG file created with record count 4028\n",
      "PSA file created with record count 4028\n",
      "PHM file created with record count 4028\n",
      "PVH file created with record count 4028\n",
      "QRVO file created with record count 1511\n",
      "QCOM file created with record count 4028\n",
      "PWR file created with record count 4028\n",
      "DGX file created with record count 4028\n",
      "RL file created with record count 4028\n",
      "RJF file created with record count 4028\n",
      "RTX file created with record count 463\n",
      "O file created with record count 4028\n",
      "REG file created with record count 4028\n",
      "REGN file created with record count 4027\n",
      "RF file created with record count 4028\n",
      "RSG file created with record count 4028\n",
      "RMD file created with record count 4028\n",
      "RHI file created with record count 4028\n",
      "ROK file created with record count 4028\n",
      "ROL file created with record count 4028\n",
      "ROP file created with record count 4028\n",
      "ROST file created with record count 4028\n",
      "RCL file created with record count 4028\n",
      "SPGI file created with record count 1179\n",
      "CRM file created with record count 4028\n",
      "SBAC file created with record count 4028\n",
      "SLB file created with record count 4028\n",
      "STX file created with record count 4028\n",
      "SEE file created with record count 4028\n",
      "SRE file created with record count 4028\n",
      "NOW file created with record count 2842\n",
      "SHW file created with record count 4028\n",
      "SPG file created with record count 4028\n",
      "SWKS file created with record count 4028\n",
      "SLG file created with record count 4028\n",
      "SNA file created with record count 4028\n",
      "SO file created with record count 4028\n",
      "LUV file created with record count 4028\n",
      "SWK file created with record count 4028\n",
      "SBUX file created with record count 4028\n",
      "STT file created with record count 4028\n",
      "STE file created with record count 4028\n",
      "SYK file created with record count 4028\n",
      "SIVB file created with record count 3927\n",
      "SYF file created with record count 1618\n",
      "SNPS file created with record count 4028\n",
      "SYY file created with record count 4028\n",
      "TMUS file created with record count 1933\n",
      "TROW file created with record count 4028\n",
      "TTWO file created with record count 4028\n",
      "TPR file created with record count 939\n",
      "TGT file created with record count 4028\n",
      "TEL file created with record count 3401\n",
      "FTI file created with record count 4028\n",
      "TDY file created with record count 4028\n",
      "TFX file created with record count 4028\n",
      "TXN file created with record count 4028\n",
      "TXT file created with record count 4028\n",
      "BK file created with record count 4028\n",
      "CLX file created with record count 4028\n",
      "COO file created with record count 4028\n",
      "HSY file created with record count 4028\n",
      "MOS file created with record count 4028\n",
      "TRV file created with record count 3488\n",
      "DIS file created with record count 4028\n",
      "TMO file created with record count 4028\n",
      "TIF file created with record count 4028\n",
      "TJX file created with record count 4028\n",
      "TSCO file created with record count 4028\n",
      "TT file created with record count 357\n",
      "TDG file created with record count 3727\n",
      "TFC file created with record count 1965\n",
      "TWTR file created with record count 2420\n",
      "TYL file created with record count 4028\n",
      "TSN file created with record count 4028\n",
      "USB file created with record count 4028\n",
      "UDR file created with record count 4028\n",
      "ULTA file created with record count 3320\n",
      "UAA file created with record count 1024\n",
      "UA file created with record count 3534\n",
      "UNP file created with record count 4028\n",
      "UAL file created with record count 2581\n",
      "UNH file created with record count 4028\n",
      "UPS file created with record count 4028\n",
      "URI file created with record count 4028\n",
      "UHS file created with record count 4028\n",
      "UNM file created with record count 4028\n",
      "VLO file created with record count 4028\n",
      "VAR file created with record count 4028\n",
      "VTR file created with record count 4028\n",
      "VRSN file created with record count 4028\n",
      "VRSK file created with record count 2829\n",
      "VZ file created with record count 4028\n",
      "VRTX file created with record count 4026\n",
      "VFC file created with record count 4028\n",
      "VIAC file created with record count 981\n",
      "V file created with record count 3621\n",
      "VNO file created with record count 4028\n",
      "VMC file created with record count 4028\n",
      "WRB file created with record count 3203\n",
      "WAB file created with record count 4028\n",
      "WBA file created with record count 1512\n",
      "WMT file created with record count 4028\n",
      "WM file created with record count 3814\n",
      "WAT file created with record count 4028\n",
      "WEC file created with record count 4028\n",
      "WFC file created with record count 4028\n",
      "WELL file created with record count 717\n",
      "WST file created with record count 4028\n",
      "WDC file created with record count 4028\n",
      "WU file created with record count 3588\n",
      "WRK file created with record count 1386\n",
      "WY file created with record count 4028\n",
      "WHR file created with record count 4028\n",
      "WMB file created with record count 4028\n",
      "WLTW file created with record count 1258\n",
      "WYNN file created with record count 4028\n",
      "XEL file created with record count 4028\n",
      "XRX file created with record count 4028\n",
      "XLNX file created with record count 4028\n",
      "XYL file created with record count 2307\n",
      "YUM file created with record count 4028\n",
      "ZBRA file created with record count 4028\n",
      "ZBH file created with record count 1389\n",
      "ZION file created with record count 4028\n",
      "ZTS file created with record count 1994\n",
      "split file for MMM 1\n",
      "split file for AOS 4\n",
      "split file for ABT 3\n",
      "No data for symbol ABBV\n",
      "split file for ABMD 1\n",
      "No data for symbol ACN\n",
      "split file for ATVI 6\n",
      "split file for ADBE 3\n",
      "split file for AAP 2\n",
      "split file for AMD 1\n",
      "split file for AES 1\n",
      "split file for AFL 4\n",
      "No data for symbol A\n",
      "split file for APD 1\n",
      "No data for symbol AKAM\n",
      "split file for ALK 2\n",
      "split file for ALB 1\n",
      "split file for ARE 1\n",
      "split file for ALXN 2\n",
      "No data for symbol ALGN\n",
      "No data for symbol ALLE\n",
      "split file for LNT 1\n",
      "split file for ALL 1\n",
      "split file for GOOGL 1\n",
      "split file for GOOG 2\n",
      "No data for symbol MO\n",
      "split file for AMZN 3\n",
      "No data for symbol AMCR\n",
      "No data for symbol AEE\n",
      "No data for symbol AAL\n",
      "No data for symbol AEP\n",
      "split file for AXP 2\n",
      "split file for AIG 4\n",
      "No data for symbol AMT\n",
      "No data for symbol AWK\n",
      "No data for symbol AMP\n",
      "split file for ABC 3\n",
      "split file for AME 4\n",
      "split file for AMGN 2\n",
      "split file for APH 4\n",
      "split file for ADI 1\n",
      "split file for ANSS 2\n",
      "split file for ANTM 1\n",
      "split file for AON 1\n",
      "split file for APA 3\n",
      "split file for AIV 3\n",
      "split file for AAPL 4\n",
      "split file for AMAT 2\n",
      "No data for symbol APTV\n",
      "split file for ADM 4\n",
      "No data for symbol ANET\n",
      "split file for AJG 2\n",
      "No data for symbol AIZ\n",
      "split file for T 1\n",
      "No data for symbol ATO\n",
      "split file for ADSK 2\n",
      "split file for ADP 2\n",
      "No data for symbol AZO\n",
      "split file for AVB 1\n",
      "No data for symbol AVY\n",
      "No data for symbol BKR\n",
      "split file for BLL 4\n",
      "split file for BAC 1\n",
      "split file for BAX 2\n",
      "split file for BDX 1\n",
      "split file for BRK.B 1\n",
      "split file for BBY 4\n",
      "split file for BIO 1\n",
      "split file for BIIB 2\n",
      "No data for symbol BLK\n",
      "No data for symbol BA\n",
      "split file for BKNG 1\n",
      "split file for BWA 3\n",
      "No data for symbol BXP\n",
      "split file for BSX 2\n",
      "split file for BMY 2\n",
      "No data for symbol AVGO\n",
      "No data for symbol BR\n",
      "split file for BF.B 3\n",
      "split file for CHRW 2\n",
      "split file for COG 4\n",
      "No data for symbol CDNS\n",
      "No data for symbol CPB\n",
      "split file for COF 1\n",
      "split file for CAH 2\n",
      "split file for KMX 1\n",
      "split file for CCL 1\n",
      "No data for symbol CARR\n",
      "split file for CAT 1\n",
      "No data for symbol CBOE\n",
      "split file for CBRE 1\n",
      "No data for symbol CDW\n",
      "No data for symbol CE\n",
      "split file for CNC 4\n",
      "split file for CNP 1\n",
      "split file for CTL 2\n",
      "split file for CERN 3\n",
      "split file for CF 1\n",
      "split file for SCHW 3\n",
      "No data for symbol CHTR\n",
      "split file for CVX 1\n",
      "No data for symbol CMG\n",
      "split file for CB 2\n",
      "split file for CHD 4\n",
      "split file for CI 2\n",
      "split file for CINF 3\n",
      "split file for CTAS 1\n",
      "split file for CSCO 3\n",
      "split file for C 3\n",
      "No data for symbol CFG\n",
      "split file for CTXS 4\n",
      "split file for CME 1\n",
      "No data for symbol CMS\n",
      "split file for KO 1\n",
      "split file for CTSH 5\n",
      "split file for CL 2\n",
      "split file for CMCSA 3\n",
      "split file for CMA 1\n",
      "No data for symbol CAG\n",
      "No data for symbol CXO\n",
      "split file for COP 2\n",
      "No data for symbol ED\n",
      "split file for STZ 3\n",
      "split file for CPRT 6\n",
      "split file for GLW 1\n",
      "No data for symbol CTVA\n",
      "split file for COST 1\n",
      "No data for symbol COTY\n",
      "No data for symbol CCI\n",
      "split file for CSX 2\n",
      "split file for CMI 2\n",
      "split file for CVS 2\n",
      "split file for DHI 5\n",
      "split file for DHR 3\n",
      "split file for DRI 1\n",
      "split file for DVA 2\n",
      "split file for DE 1\n",
      "No data for symbol DAL\n",
      "split file for XRAY 2\n",
      "split file for DVN 1\n",
      "No data for symbol DXCM\n",
      "No data for symbol FANG\n",
      "No data for symbol DLR\n",
      "No data for symbol DFS\n",
      "No data for symbol DISCA\n",
      "No data for symbol DISCK\n",
      "split file for DISH 3\n",
      "No data for symbol DG\n",
      "split file for DLTR 4\n",
      "split file for D 1\n",
      "No data for symbol DPZ\n",
      "No data for symbol DOV\n",
      "No data for symbol DOW\n",
      "No data for symbol DTE\n",
      "split file for DUK 3\n",
      "No data for symbol DRE\n",
      "split file for DD 1\n",
      "split file for DXC 1\n",
      "split file for ETFC 3\n",
      "split file for EMN 1\n",
      "split file for ETN 3\n",
      "split file for EBAY 4\n",
      "split file for ECL 2\n",
      "No data for symbol EIX\n",
      "split file for EW 3\n",
      "split file for EA 2\n",
      "split file for EMR 1\n",
      "No data for symbol ETR\n",
      "split file for EOG 2\n",
      "No data for symbol EFX\n",
      "split file for EQIX 1\n",
      "split file for EQR 1\n",
      "No data for symbol ESS\n",
      "split file for EL 2\n",
      "No data for symbol RE\n",
      "No data for symbol EVRG\n",
      "split file for ES 1\n",
      "split file for EXC 1\n",
      "split file for EXPE 1\n",
      "split file for EXPD 3\n",
      "No data for symbol EXR\n",
      "split file for XOM 1\n",
      "split file for FFIV 1\n",
      "No data for symbol FB\n",
      "split file for FAST 4\n",
      "No data for symbol FRT\n",
      "split file for FDX 1\n",
      "No data for symbol FIS\n",
      "split file for FITB 2\n",
      "No data for symbol FRC\n",
      "split file for FE 1\n",
      "split file for FISV 5\n",
      "No data for symbol FLT\n",
      "split file for FLIR 3\n",
      "split file for FLS 1\n",
      "split file for FMC 2\n",
      "split file for F 3\n",
      "split file for FTNT 1\n",
      "split file for FTV 1\n",
      "No data for symbol FBHS\n",
      "No data for symbol FOXA\n",
      "No data for symbol FOX\n",
      "split file for BEN 2\n",
      "split file for FCX 1\n",
      "split file for GPS 2\n",
      "split file for GRMN 1\n",
      "No data for symbol IT\n",
      "split file for GD 2\n",
      "split file for GE 1\n",
      "split file for GIS 2\n",
      "No data for symbol GM\n",
      "No data for symbol GPC\n",
      "split file for GILD 5\n",
      "split file for GPN 2\n",
      "split file for GL 2\n",
      "No data for symbol GS\n",
      "split file for GWW 1\n",
      "split file for HRB 2\n",
      "split file for HAL 1\n",
      "split file for HBI 1\n",
      "split file for HIG 1\n",
      "split file for HAS 1\n",
      "No data for symbol HCA\n",
      "split file for PEAK 1\n",
      "split file for HSIC 2\n",
      "split file for HES 1\n",
      "No data for symbol HPE\n",
      "split file for HLT 2\n",
      "split file for HFC 4\n",
      "split file for HOLX 2\n",
      "split file for HD 2\n",
      "No data for symbol HON\n",
      "split file for HRL 3\n",
      "split file for HST 1\n",
      "No data for symbol HWM\n",
      "split file for HPQ 1\n",
      "No data for symbol HUM\n",
      "split file for HBAN 3\n",
      "No data for symbol HII\n",
      "split file for IEX 2\n",
      "split file for IDXX 2\n",
      "No data for symbol INFO\n",
      "split file for ITW 1\n",
      "split file for ILMN 1\n",
      "split file for INCY 1\n",
      "split file for IR 2\n",
      "split file for INTC 2\n",
      "split file for ICE 1\n",
      "split file for IBM 1\n",
      "No data for symbol IFF\n",
      "No data for symbol IP\n",
      "split file for IPG 1\n",
      "split file for INTU 2\n",
      "split file for ISRG 2\n",
      "split file for IVZ 3\n",
      "No data for symbol IPGP\n",
      "No data for symbol IQV\n",
      "split file for IRM 4\n",
      "split file for JBHT 2\n",
      "split file for JKHY 2\n",
      "split file for J 2\n",
      "split file for SJM 1\n",
      "split file for JNJ 1\n",
      "split file for JCI 5\n",
      "split file for JPM 2\n",
      "split file for JNPR 2\n",
      "split file for KSU 1\n",
      "No data for symbol K\n",
      "split file for KEY 1\n",
      "No data for symbol KEYS\n",
      "No data for symbol KMB\n",
      "split file for KIM 2\n",
      "No data for symbol KMI\n",
      "split file for KLAC 1\n",
      "split file for KSS 2\n",
      "No data for symbol KHC\n",
      "split file for KR 2\n",
      "split file for LB 2\n",
      "split file for LHX 2\n",
      "split file for LH 3\n",
      "split file for LRCX 1\n",
      "No data for symbol LW\n",
      "No data for symbol LVS\n",
      "split file for LEG 1\n",
      "split file for LDOS 1\n",
      "split file for LEN 1\n",
      "No data for symbol LLY\n",
      "split file for LNC 1\n",
      "split file for LIN 1\n",
      "No data for symbol LYV\n",
      "split file for LKQ 3\n",
      "split file for LMT 1\n",
      "split file for L 2\n",
      "split file for LOW 3\n",
      "No data for symbol LYB\n",
      "split file for MTB 1\n",
      "split file for MRO 2\n",
      "split file for MPC 1\n",
      "No data for symbol MKTX\n",
      "split file for MAR 5\n",
      "split file for MMC 2\n",
      "No data for symbol MLM\n",
      "split file for MAS 1\n",
      "split file for MA 1\n",
      "split file for MXIM 1\n",
      "split file for MKC 2\n",
      "split file for MCD 1\n",
      "split file for MCK 1\n",
      "split file for MDT 1\n",
      "split file for MRK 1\n",
      "No data for symbol MET\n",
      "No data for symbol MTD\n",
      "split file for MGM 2\n",
      "split file for MCHP 3\n",
      "split file for MU 1\n",
      "split file for MSFT 3\n",
      "No data for symbol MAA\n",
      "No data for symbol MHK\n",
      "split file for TAP 1\n",
      "No data for symbol MDLZ\n",
      "split file for MNST 4\n",
      "split file for MCO 1\n",
      "split file for MS 1\n",
      "split file for MSI 3\n",
      "No data for symbol MSCI\n",
      "split file for MYL 2\n",
      "No data for symbol NDAQ\n",
      "split file for NOV 1\n",
      "split file for NTAP 3\n",
      "split file for NFLX 2\n",
      "No data for symbol NWL\n",
      "No data for symbol NEM\n",
      "No data for symbol NWSA\n",
      "No data for symbol NWS\n",
      "split file for NEE 2\n",
      "No data for symbol NLSN\n",
      "split file for NKE 3\n",
      "split file for NI 1\n",
      "split file for NBL 2\n",
      "No data for symbol NSC\n",
      "split file for NTRS 1\n",
      "split file for NOC 2\n",
      "No data for symbol NLOK\n",
      "No data for symbol NCLH\n",
      "split file for NRG 1\n",
      "split file for NUE 2\n",
      "split file for NVDA 4\n",
      "No data for symbol NVR\n",
      "split file for ORLY 2\n",
      "split file for OXY 1\n",
      "split file for ODFL 6\n",
      "split file for OMC 1\n",
      "split file for OKE 2\n",
      "split file for ORCL 3\n",
      "No data for symbol OTIS\n",
      "split file for PCAR 4\n",
      "No data for symbol PKG\n",
      "split file for PH 1\n",
      "split file for PAYX 3\n",
      "No data for symbol PAYC\n",
      "No data for symbol PYPL\n",
      "split file for PNR 1\n",
      "split file for PBCT 3\n",
      "No data for symbol PEP\n",
      "split file for PKI 1\n",
      "No data for symbol PRGO\n",
      "split file for PFE 1\n",
      "No data for symbol PM\n",
      "No data for symbol PSX\n",
      "split file for PNW 1\n",
      "No data for symbol PXD\n",
      "No data for symbol PNC\n",
      "split file for PPG 1\n",
      "split file for PPL 1\n",
      "No data for symbol PFG\n",
      "split file for PG 1\n",
      "split file for PGR 2\n",
      "No data for symbol PLD\n",
      "No data for symbol PRU\n",
      "split file for PEG 1\n",
      "No data for symbol PSA\n",
      "split file for PHM 3\n",
      "No data for symbol PVH\n",
      "No data for symbol QRVO\n",
      "split file for QCOM 3\n",
      "split file for PWR 1\n",
      "split file for DGX 2\n",
      "No data for symbol RL\n",
      "split file for RJF 3\n",
      "No data for symbol RTX\n",
      "split file for O 1\n",
      "No data for symbol REG\n",
      "No data for symbol REGN\n",
      "split file for RF 1\n",
      "split file for RSG 1\n",
      "split file for RMD 4\n",
      "split file for RHI 1\n",
      "No data for symbol ROK\n",
      "split file for ROL 7\n",
      "split file for ROP 1\n",
      "split file for ROST 4\n",
      "split file for RCL 1\n",
      "split file for SPGI 2\n",
      "split file for CRM 1\n",
      "No data for symbol SBAC\n",
      "split file for SLB 1\n",
      "No data for symbol STX\n",
      "split file for SEE 1\n",
      "No data for symbol SRE\n",
      "No data for symbol NOW\n",
      "No data for symbol SHW\n",
      "split file for SPG 2\n",
      "split file for SWKS 2\n",
      "No data for symbol SLG\n",
      "No data for symbol SNA\n",
      "split file for SO 1\n",
      "split file for LUV 3\n",
      "No data for symbol SWK\n",
      "split file for SBUX 4\n",
      "split file for STT 1\n",
      "split file for STE 1\n",
      "split file for SYK 2\n",
      "split file for SIVB 2\n",
      "No data for symbol SYF\n",
      "split file for SNPS 1\n",
      "split file for SYY 2\n",
      "split file for TMUS 1\n",
      "split file for TROW 2\n",
      "split file for TTWO 1\n",
      "split file for TPR 3\n",
      "split file for TGT 2\n",
      "No data for symbol TEL\n",
      "split file for FTI 2\n",
      "No data for symbol TDY\n",
      "No data for symbol TFX\n",
      "split file for TXN 2\n",
      "split file for TXT 1\n",
      "split file for BK 2\n",
      "split file for CLX 1\n",
      "split file for COO 1\n",
      "split file for HSY 1\n",
      "No data for symbol MOS\n",
      "split file for TRV 1\n",
      "split file for DIS 2\n",
      "No data for symbol TMO\n",
      "split file for TIF 2\n",
      "split file for TJX 4\n",
      "split file for TSCO 4\n",
      "No data for symbol TT\n",
      "No data for symbol TDG\n",
      "split file for TFC 1\n",
      "No data for symbol TWTR\n",
      "No data for symbol TYL\n",
      "No data for symbol TSN\n",
      "split file for USB 3\n",
      "No data for symbol UDR\n",
      "No data for symbol ULTA\n",
      "split file for UAA 1\n",
      "split file for UA 2\n",
      "split file for UNP 2\n",
      "No data for symbol UAL\n",
      "split file for UNH 3\n",
      "No data for symbol UPS\n",
      "No data for symbol URI\n",
      "split file for UHS 2\n",
      "No data for symbol UNM\n",
      "split file for VLO 3\n",
      "split file for VAR 3\n",
      "No data for symbol VTR\n",
      "split file for VRSN 2\n",
      "No data for symbol VRSK\n",
      "split file for VZ 4\n",
      "split file for VRTX 1\n",
      "split file for VFC 1\n",
      "No data for symbol VIAC\n",
      "split file for V 1\n",
      "split file for VNO 1\n",
      "split file for VMC 1\n",
      "split file for WRB 5\n",
      "split file for WAB 1\n",
      "split file for WBA 1\n",
      "split file for WMT 1\n",
      "No data for symbol WM\n",
      "split file for WAT 2\n",
      "split file for WEC 1\n",
      "split file for WFC 1\n",
      "No data for symbol WELL\n",
      "split file for WST 2\n",
      "No data for symbol WDC\n",
      "No data for symbol WU\n",
      "No data for symbol WRK\n",
      "split file for WY 1\n",
      "No data for symbol WHR\n",
      "split file for WMB 2\n",
      "No data for symbol WLTW\n",
      "No data for symbol WYNN\n",
      "split file for XEL 1\n",
      "split file for XRX 2\n",
      "split file for XLNX 2\n",
      "No data for symbol XYL\n",
      "split file for YUM 2\n",
      "split file for ZBRA 2\n",
      "No data for symbol ZBH\n",
      "No data for symbol ZION\n",
      "No data for symbol ZTS\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'split_corrections.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fd159b57ee0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/splits'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Fix data for about 50 splits from a correction file created manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfix_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/splits'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#%%  Pull in all the dividend data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-db4c6224c5f3>\u001b[0m in \u001b[0;36mfix_splits\u001b[1;34m(splitpath)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfix_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;31m# Get the split corrections to overwrite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mcorrect_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'split_corrections.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;31m# create a list of symbols to fix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0msymbols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'split_corrections.csv'"
     ]
    }
   ],
   "source": [
    "#%%  Filter down to the tickers I'm interestead in (this could also be done by modifying get_tickers)\n",
    "# symbols = filter_us_exch(symbols)\n",
    "\n",
    "symbols = list(sp_constituents.iloc[:, -1])\n",
    "\n",
    "#%% Get all the aggregated bar/pricing data for each symbol in the filtered list\n",
    "get_bars(symbols, 'data/bars', START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "split file for MMM 1\n",
      "split file for AOS 4\n",
      "split file for ABT 3\n",
      "No data for symbol ABBV\n",
      "split file for ABMD 1\n",
      "No data for symbol ACN\n",
      "split file for ATVI 6\n",
      "split file for ADBE 3\n",
      "split file for AAP 2\n",
      "split file for AMD 1\n",
      "split file for AES 1\n",
      "split file for AFL 4\n",
      "No data for symbol A\n",
      "split file for APD 1\n",
      "No data for symbol AKAM\n",
      "split file for ALK 2\n",
      "split file for ALB 1\n",
      "split file for ARE 1\n",
      "split file for ALXN 2\n",
      "No data for symbol ALGN\n",
      "No data for symbol ALLE\n",
      "split file for LNT 1\n",
      "split file for ALL 1\n",
      "split file for GOOGL 1\n",
      "split file for GOOG 2\n",
      "No data for symbol MO\n",
      "split file for AMZN 3\n",
      "No data for symbol AMCR\n",
      "No data for symbol AEE\n",
      "No data for symbol AAL\n",
      "No data for symbol AEP\n",
      "split file for AXP 2\n",
      "split file for AIG 4\n",
      "No data for symbol AMT\n",
      "No data for symbol AWK\n",
      "No data for symbol AMP\n",
      "split file for ABC 3\n",
      "split file for AME 4\n",
      "split file for AMGN 2\n",
      "split file for APH 4\n",
      "split file for ADI 1\n",
      "split file for ANSS 2\n",
      "split file for ANTM 1\n",
      "split file for AON 1\n",
      "split file for APA 3\n",
      "split file for AIV 3\n",
      "split file for AAPL 4\n",
      "split file for AMAT 2\n",
      "No data for symbol APTV\n",
      "split file for ADM 4\n",
      "No data for symbol ANET\n",
      "split file for AJG 2\n",
      "No data for symbol AIZ\n",
      "split file for T 1\n",
      "No data for symbol ATO\n",
      "split file for ADSK 2\n",
      "split file for ADP 2\n",
      "No data for symbol AZO\n",
      "split file for AVB 1\n",
      "No data for symbol AVY\n",
      "No data for symbol BKR\n",
      "split file for BLL 4\n",
      "split file for BAC 1\n",
      "split file for BAX 2\n",
      "split file for BDX 1\n",
      "split file for BRK.B 1\n",
      "split file for BBY 4\n",
      "split file for BIO 1\n",
      "split file for BIIB 2\n",
      "No data for symbol BLK\n",
      "No data for symbol BA\n",
      "split file for BKNG 1\n",
      "split file for BWA 3\n",
      "No data for symbol BXP\n",
      "split file for BSX 2\n",
      "split file for BMY 2\n",
      "No data for symbol AVGO\n",
      "No data for symbol BR\n",
      "split file for BF.B 3\n",
      "split file for CHRW 2\n",
      "split file for COG 4\n",
      "No data for symbol CDNS\n",
      "No data for symbol CPB\n",
      "split file for COF 1\n",
      "split file for CAH 2\n",
      "split file for KMX 1\n",
      "split file for CCL 1\n",
      "No data for symbol CARR\n",
      "split file for CAT 1\n",
      "No data for symbol CBOE\n",
      "split file for CBRE 1\n",
      "No data for symbol CDW\n",
      "No data for symbol CE\n",
      "split file for CNC 4\n",
      "split file for CNP 1\n",
      "split file for CTL 2\n",
      "split file for CERN 3\n",
      "split file for CF 1\n",
      "split file for SCHW 3\n",
      "No data for symbol CHTR\n",
      "split file for CVX 1\n",
      "No data for symbol CMG\n",
      "split file for CB 2\n",
      "split file for CHD 4\n",
      "split file for CI 2\n",
      "split file for CINF 3\n",
      "split file for CTAS 1\n",
      "split file for CSCO 3\n",
      "split file for C 3\n",
      "No data for symbol CFG\n",
      "split file for CTXS 4\n",
      "split file for CME 1\n",
      "No data for symbol CMS\n",
      "split file for KO 1\n",
      "split file for CTSH 5\n",
      "split file for CL 2\n",
      "split file for CMCSA 3\n",
      "split file for CMA 1\n",
      "No data for symbol CAG\n",
      "No data for symbol CXO\n",
      "split file for COP 2\n",
      "No data for symbol ED\n",
      "split file for STZ 3\n",
      "split file for CPRT 6\n",
      "split file for GLW 1\n",
      "No data for symbol CTVA\n",
      "split file for COST 1\n",
      "No data for symbol COTY\n",
      "No data for symbol CCI\n",
      "split file for CSX 2\n",
      "split file for CMI 2\n",
      "split file for CVS 2\n",
      "split file for DHI 5\n",
      "split file for DHR 3\n",
      "split file for DRI 1\n",
      "split file for DVA 2\n",
      "split file for DE 1\n",
      "No data for symbol DAL\n",
      "split file for XRAY 2\n",
      "split file for DVN 1\n",
      "No data for symbol DXCM\n",
      "No data for symbol FANG\n",
      "No data for symbol DLR\n",
      "No data for symbol DFS\n",
      "No data for symbol DISCA\n",
      "No data for symbol DISCK\n",
      "split file for DISH 3\n",
      "No data for symbol DG\n",
      "split file for DLTR 4\n",
      "split file for D 1\n",
      "No data for symbol DPZ\n",
      "No data for symbol DOV\n",
      "No data for symbol DOW\n",
      "No data for symbol DTE\n",
      "split file for DUK 3\n",
      "No data for symbol DRE\n",
      "split file for DD 1\n",
      "split file for DXC 1\n",
      "split file for ETFC 3\n",
      "split file for EMN 1\n",
      "split file for ETN 3\n",
      "split file for EBAY 4\n",
      "split file for ECL 2\n",
      "No data for symbol EIX\n",
      "split file for EW 3\n",
      "split file for EA 2\n",
      "split file for EMR 1\n",
      "No data for symbol ETR\n",
      "split file for EOG 2\n",
      "No data for symbol EFX\n",
      "split file for EQIX 1\n",
      "split file for EQR 1\n",
      "No data for symbol ESS\n",
      "split file for EL 2\n",
      "No data for symbol RE\n",
      "No data for symbol EVRG\n",
      "split file for ES 1\n",
      "split file for EXC 1\n",
      "split file for EXPE 1\n",
      "split file for EXPD 3\n",
      "No data for symbol EXR\n",
      "split file for XOM 1\n",
      "split file for FFIV 1\n",
      "No data for symbol FB\n",
      "split file for FAST 4\n",
      "No data for symbol FRT\n",
      "split file for FDX 1\n",
      "No data for symbol FIS\n",
      "split file for FITB 2\n",
      "No data for symbol FRC\n",
      "split file for FE 1\n",
      "split file for FISV 5\n",
      "No data for symbol FLT\n",
      "split file for FLIR 3\n",
      "split file for FLS 1\n",
      "split file for FMC 2\n",
      "split file for F 3\n",
      "split file for FTNT 1\n",
      "split file for FTV 1\n",
      "No data for symbol FBHS\n",
      "No data for symbol FOXA\n",
      "No data for symbol FOX\n",
      "split file for BEN 2\n",
      "split file for FCX 1\n",
      "split file for GPS 2\n",
      "split file for GRMN 1\n",
      "No data for symbol IT\n",
      "split file for GD 2\n",
      "split file for GE 1\n",
      "split file for GIS 2\n",
      "No data for symbol GM\n",
      "No data for symbol GPC\n",
      "split file for GILD 5\n",
      "split file for GPN 2\n",
      "split file for GL 2\n",
      "No data for symbol GS\n",
      "split file for GWW 1\n",
      "split file for HRB 2\n",
      "split file for HAL 1\n",
      "split file for HBI 1\n",
      "split file for HIG 1\n",
      "split file for HAS 1\n",
      "No data for symbol HCA\n",
      "split file for PEAK 1\n",
      "split file for HSIC 2\n",
      "split file for HES 1\n",
      "No data for symbol HPE\n",
      "split file for HLT 2\n",
      "split file for HFC 4\n",
      "split file for HOLX 2\n",
      "split file for HD 2\n",
      "No data for symbol HON\n",
      "split file for HRL 3\n",
      "split file for HST 1\n",
      "No data for symbol HWM\n",
      "split file for HPQ 1\n",
      "No data for symbol HUM\n",
      "split file for HBAN 3\n",
      "No data for symbol HII\n",
      "split file for IEX 2\n",
      "split file for IDXX 2\n",
      "No data for symbol INFO\n",
      "split file for ITW 1\n",
      "split file for ILMN 1\n",
      "split file for INCY 1\n",
      "split file for IR 2\n",
      "split file for INTC 2\n",
      "split file for ICE 1\n",
      "split file for IBM 1\n",
      "No data for symbol IFF\n",
      "No data for symbol IP\n",
      "split file for IPG 1\n",
      "split file for INTU 2\n",
      "split file for ISRG 2\n",
      "split file for IVZ 3\n",
      "No data for symbol IPGP\n",
      "No data for symbol IQV\n",
      "split file for IRM 4\n",
      "split file for JBHT 2\n",
      "split file for JKHY 2\n",
      "split file for J 2\n",
      "split file for SJM 1\n",
      "split file for JNJ 1\n",
      "split file for JCI 5\n",
      "split file for JPM 2\n",
      "split file for JNPR 2\n",
      "split file for KSU 1\n",
      "No data for symbol K\n",
      "split file for KEY 1\n",
      "No data for symbol KEYS\n",
      "No data for symbol KMB\n",
      "split file for KIM 2\n",
      "No data for symbol KMI\n",
      "split file for KLAC 1\n",
      "split file for KSS 2\n",
      "No data for symbol KHC\n",
      "split file for KR 2\n",
      "split file for LB 2\n",
      "split file for LHX 2\n",
      "split file for LH 3\n",
      "split file for LRCX 1\n",
      "No data for symbol LW\n",
      "No data for symbol LVS\n",
      "split file for LEG 1\n",
      "split file for LDOS 1\n",
      "split file for LEN 1\n",
      "No data for symbol LLY\n",
      "split file for LNC 1\n",
      "split file for LIN 1\n",
      "No data for symbol LYV\n",
      "split file for LKQ 3\n",
      "split file for LMT 1\n",
      "split file for L 2\n",
      "split file for LOW 3\n",
      "No data for symbol LYB\n",
      "split file for MTB 1\n",
      "split file for MRO 2\n",
      "split file for MPC 1\n",
      "No data for symbol MKTX\n",
      "split file for MAR 5\n",
      "split file for MMC 2\n",
      "No data for symbol MLM\n",
      "split file for MAS 1\n",
      "split file for MA 1\n",
      "split file for MXIM 1\n",
      "split file for MKC 2\n",
      "split file for MCD 1\n",
      "split file for MCK 1\n",
      "split file for MDT 1\n",
      "split file for MRK 1\n",
      "No data for symbol MET\n",
      "No data for symbol MTD\n",
      "split file for MGM 2\n",
      "split file for MCHP 3\n",
      "split file for MU 1\n",
      "split file for MSFT 3\n",
      "No data for symbol MAA\n",
      "No data for symbol MHK\n",
      "split file for TAP 1\n",
      "No data for symbol MDLZ\n",
      "split file for MNST 4\n",
      "split file for MCO 1\n",
      "split file for MS 1\n",
      "split file for MSI 3\n",
      "No data for symbol MSCI\n",
      "split file for MYL 2\n",
      "No data for symbol NDAQ\n",
      "split file for NOV 1\n",
      "split file for NTAP 3\n",
      "split file for NFLX 2\n",
      "No data for symbol NWL\n",
      "No data for symbol NEM\n",
      "No data for symbol NWSA\n",
      "No data for symbol NWS\n",
      "split file for NEE 2\n",
      "No data for symbol NLSN\n",
      "split file for NKE 3\n",
      "split file for NI 1\n",
      "split file for NBL 2\n",
      "No data for symbol NSC\n",
      "split file for NTRS 1\n",
      "split file for NOC 2\n",
      "No data for symbol NLOK\n",
      "No data for symbol NCLH\n",
      "split file for NRG 1\n",
      "split file for NUE 2\n",
      "split file for NVDA 4\n",
      "No data for symbol NVR\n",
      "split file for ORLY 2\n",
      "split file for OXY 1\n",
      "split file for ODFL 6\n",
      "split file for OMC 1\n",
      "split file for OKE 2\n",
      "split file for ORCL 3\n",
      "No data for symbol OTIS\n",
      "split file for PCAR 4\n",
      "No data for symbol PKG\n",
      "split file for PH 1\n",
      "split file for PAYX 3\n",
      "No data for symbol PAYC\n",
      "No data for symbol PYPL\n",
      "split file for PNR 1\n",
      "split file for PBCT 3\n",
      "No data for symbol PEP\n",
      "split file for PKI 1\n",
      "No data for symbol PRGO\n",
      "split file for PFE 1\n",
      "No data for symbol PM\n",
      "No data for symbol PSX\n",
      "split file for PNW 1\n",
      "No data for symbol PXD\n",
      "No data for symbol PNC\n",
      "split file for PPG 1\n",
      "split file for PPL 1\n",
      "No data for symbol PFG\n",
      "split file for PG 1\n",
      "split file for PGR 2\n",
      "No data for symbol PLD\n",
      "No data for symbol PRU\n",
      "split file for PEG 1\n",
      "No data for symbol PSA\n",
      "split file for PHM 3\n",
      "No data for symbol PVH\n",
      "No data for symbol QRVO\n",
      "split file for QCOM 3\n",
      "split file for PWR 1\n",
      "split file for DGX 2\n",
      "No data for symbol RL\n",
      "split file for RJF 3\n",
      "No data for symbol RTX\n",
      "split file for O 1\n",
      "No data for symbol REG\n",
      "No data for symbol REGN\n",
      "split file for RF 1\n",
      "split file for RSG 1\n",
      "split file for RMD 4\n",
      "split file for RHI 1\n",
      "No data for symbol ROK\n",
      "split file for ROL 7\n",
      "split file for ROP 1\n",
      "split file for ROST 4\n",
      "split file for RCL 1\n",
      "split file for SPGI 2\n",
      "split file for CRM 1\n",
      "No data for symbol SBAC\n",
      "split file for SLB 1\n",
      "No data for symbol STX\n",
      "split file for SEE 1\n",
      "No data for symbol SRE\n",
      "No data for symbol NOW\n",
      "No data for symbol SHW\n",
      "split file for SPG 2\n",
      "split file for SWKS 2\n",
      "No data for symbol SLG\n",
      "No data for symbol SNA\n",
      "split file for SO 1\n",
      "split file for LUV 3\n",
      "No data for symbol SWK\n",
      "split file for SBUX 4\n",
      "split file for STT 1\n",
      "split file for STE 1\n",
      "split file for SYK 2\n",
      "split file for SIVB 2\n",
      "No data for symbol SYF\n",
      "split file for SNPS 1\n",
      "split file for SYY 2\n",
      "split file for TMUS 1\n",
      "split file for TROW 2\n",
      "split file for TTWO 1\n",
      "split file for TPR 3\n",
      "split file for TGT 2\n",
      "No data for symbol TEL\n",
      "split file for FTI 2\n",
      "No data for symbol TDY\n",
      "No data for symbol TFX\n",
      "split file for TXN 2\n",
      "split file for TXT 1\n",
      "split file for BK 2\n",
      "split file for CLX 1\n",
      "split file for COO 1\n",
      "split file for HSY 1\n",
      "No data for symbol MOS\n",
      "split file for TRV 1\n",
      "split file for DIS 2\n",
      "No data for symbol TMO\n",
      "split file for TIF 2\n",
      "split file for TJX 4\n",
      "split file for TSCO 4\n",
      "No data for symbol TT\n",
      "No data for symbol TDG\n",
      "split file for TFC 1\n",
      "No data for symbol TWTR\n",
      "No data for symbol TYL\n",
      "No data for symbol TSN\n",
      "split file for USB 3\n",
      "No data for symbol UDR\n",
      "No data for symbol ULTA\n",
      "split file for UAA 1\n",
      "split file for UA 2\n",
      "split file for UNP 2\n",
      "No data for symbol UAL\n",
      "split file for UNH 3\n",
      "No data for symbol UPS\n",
      "No data for symbol URI\n",
      "split file for UHS 2\n",
      "No data for symbol UNM\n",
      "split file for VLO 3\n",
      "split file for VAR 3\n",
      "No data for symbol VTR\n",
      "split file for VRSN 2\n",
      "No data for symbol VRSK\n",
      "split file for VZ 4\n",
      "split file for VRTX 1\n",
      "split file for VFC 1\n",
      "No data for symbol VIAC\n",
      "split file for V 1\n",
      "split file for VNO 1\n",
      "split file for VMC 1\n",
      "split file for WRB 5\n",
      "split file for WAB 1\n",
      "split file for WBA 1\n",
      "split file for WMT 1\n",
      "No data for symbol WM\n",
      "split file for WAT 2\n",
      "split file for WEC 1\n",
      "split file for WFC 1\n",
      "No data for symbol WELL\n",
      "split file for WST 2\n",
      "No data for symbol WDC\n",
      "No data for symbol WU\n",
      "No data for symbol WRK\n",
      "split file for WY 1\n",
      "No data for symbol WHR\n",
      "split file for WMB 2\n",
      "No data for symbol WLTW\n",
      "No data for symbol WYNN\n",
      "split file for XEL 1\n",
      "split file for XRX 2\n",
      "split file for XLNX 2\n",
      "No data for symbol XYL\n",
      "split file for YUM 2\n",
      "split file for ZBRA 2\n",
      "No data for symbol ZBH\n",
      "No data for symbol ZION\n",
      "No data for symbol ZTS\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'split_corrections.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7c2244277182>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/splits'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Fix data for about 50 splits from a correction file created manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfix_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/splits'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#%%  Pull in all the dividend data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-db4c6224c5f3>\u001b[0m in \u001b[0;36mfix_splits\u001b[1;34m(splitpath)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfix_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;31m# Get the split corrections to overwrite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mcorrect_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'split_corrections.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;31m# create a list of symbols to fix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0msymbols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dallas-real-estate-analysis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'split_corrections.csv'"
     ]
    }
   ],
   "source": [
    "#%%  Pull in all the stock splits\n",
    "get_splits(symbols, 'data/splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MA\n",
      "         date ticker  ratio  tofactor  forfactor\n",
      "0  2014-01-21     MA    0.1         1         10\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2014-01-22     MA    0.1\n",
      "Split file for MA corrected\n",
      "NFLX\n",
      "         date ticker splitDeclaredDate     ratio  tofactor  forfactor\n",
      "0  2015-07-14   NFLX        2015-06-23  0.142857       1.0        7.0\n",
      "1  2004-02-12   NFLX               NaN  0.500000       NaN        NaN\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2015-07-15   NFLX  0.143\n",
      "2004-02-12   NFLX  0.500\n",
      "Split file for NFLX corrected\n",
      "BRK.A\n",
      "no file found\n",
      "EWZ\n",
      "no file found\n",
      "JEF\n",
      "no file found\n",
      "WY\n",
      "         date ticker  ratio\n",
      "0  2010-07-19     WY    0.5\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2010-07-20     WY    0.5\n",
      "Split file for WY corrected\n",
      "STN\n",
      "no file found\n",
      "CTSH\n",
      "         date ticker splitDeclaredDate     ratio  tofactor  forfactor\n",
      "0  2014-03-06   CTSH        2014-02-04  0.500000       1.0        2.0\n",
      "1  2007-10-17   CTSH               NaN  0.500000       NaN        NaN\n",
      "2  2004-06-18   CTSH               NaN  0.500000       NaN        NaN\n",
      "3  2003-04-02   CTSH               NaN  0.333333       NaN        NaN\n",
      "4  2000-03-17   CTSH               NaN  0.500000       NaN        NaN\n",
      "           ticker     ratio\n",
      "date                       \n",
      "2014-03-10   CTSH  0.500000\n",
      "2007-10-17   CTSH  0.500000\n",
      "2004-06-18   CTSH  0.500000\n",
      "2003-04-02   CTSH  0.333333\n",
      "2000-03-17   CTSH  0.500000\n",
      "Split file for CTSH corrected\n",
      "ENB\n",
      "no file found\n",
      "TQQQ\n",
      "no file found\n",
      "MNST\n",
      "         date ticker splitDeclaredDate     ratio  tofactor  forfactor\n",
      "0  2016-11-10   MNST        2016-10-14  0.333333       1.0        3.0\n",
      "1  2012-02-16   MNST               NaN  0.500000       NaN        NaN\n",
      "2  2006-07-10   MNST               NaN  0.250000       NaN        NaN\n",
      "3  2005-08-09   MNST               NaN  0.500000       NaN        NaN\n",
      "           ticker     ratio\n",
      "date                       \n",
      "2016-11-10   MNST  0.333333\n",
      "2012-02-16   MNST  0.500000\n",
      "2006-07-10   MNST  1.000000\n",
      "2005-08-09   MNST  1.000000\n",
      "Split file for MNST corrected\n",
      "CNC\n",
      "         date ticker     ratio  tofactor  forfactor splitDeclaredDate\n",
      "0  2019-02-07    CNC  0.500000       1.0        2.0               NaN\n",
      "1  2015-02-18    CNC  0.500000       1.0        2.0        2015-02-03\n",
      "2  2004-12-20    CNC  0.500000       NaN        NaN               NaN\n",
      "3  2003-07-14    CNC  0.666667       NaN        NaN               NaN\n",
      "           ticker     ratio\n",
      "date                       \n",
      "2019-02-07    CNC  0.500000\n",
      "2015-02-20    CNC  0.500000\n",
      "2004-12-20    CNC  0.500000\n",
      "2003-07-14    CNC  0.666667\n",
      "Split file for CNC corrected\n",
      "CHD\n",
      "         date ticker     ratio  tofactor  forfactor\n",
      "0  2016-08-31    CHD  0.500000       1.0        2.0\n",
      "1  2011-06-02    CHD  0.500000       NaN        NaN\n",
      "2  2004-09-02    CHD  0.666667       NaN        NaN\n",
      "3  1999-09-02    CHD  0.500000       NaN        NaN\n",
      "           ticker     ratio\n",
      "date                       \n",
      "2016-09-02    CHD  0.500000\n",
      "2011-06-02    CHD  0.500000\n",
      "2004-09-02    CHD  0.666667\n",
      "1999-09-02    CHD  0.500000\n",
      "Split file for CHD corrected\n",
      "SPR\n",
      "no file found\n",
      "ERIC\n",
      "no file found\n",
      "PDS\n",
      "no file found\n",
      "CLR\n",
      "no file found\n",
      "GEN\n",
      "no file found\n",
      "NUGT\n",
      "no file found\n",
      "ODP\n",
      "no file found\n",
      "ITT\n",
      "no file found\n",
      "ADS\n",
      "no file found\n",
      "TZA\n",
      "no file found\n",
      "FAZ\n",
      "no file found\n",
      "VXX\n",
      "no file found\n",
      "MRO\n",
      "         date ticker   ratio\n",
      "0  2011-06-30    MRO  0.5965\n",
      "1  2007-06-19    MRO  0.5000\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2011-07-01    MRO  0.597\n",
      "2007-06-19    MRO  0.500\n",
      "Split file for MRO corrected\n",
      "JNK\n",
      "no file found\n",
      "HLT\n",
      "         date ticker splitDeclaredDate  ratio  tofactor  forfactor\n",
      "0  2017-01-04    HLT        2017-01-04      3         3          1\n",
      "1  2017-01-03    HLT        2017-01-03      3         3          1\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2017-01-04    HLT    3.0\n",
      "2017-01-03    HLT    1.0\n",
      "Split file for HLT corrected\n",
      "CAL\n",
      "no file found\n",
      "CTXS\n",
      "         date ticker     ratio  tofactor  forfactor\n",
      "0  2017-02-01   CTXS  0.500000       1.0        2.0\n",
      "1  2000-02-17   CTXS  0.500000       NaN        NaN\n",
      "2  1999-03-26   CTXS  0.500000       NaN        NaN\n",
      "3  1998-02-23   CTXS  0.666667       NaN        NaN\n",
      "           ticker     ratio\n",
      "date                       \n",
      "2017-02-01   CTXS  0.840000\n",
      "2000-02-17   CTXS  0.500000\n",
      "1999-03-26   CTXS  0.500000\n",
      "1998-02-23   CTXS  0.666667\n",
      "Split file for CTXS corrected\n",
      "GOOG\n",
      "         date ticker splitDeclaredDate  ratio  tofactor  forfactor\n",
      "0  2015-04-27   GOOG        2015-04-27      1  10000000   10027455\n",
      "1  2014-03-27   GOOG        2014-03-27      2      1000       2002\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2015-04-27   GOOG    1.0\n",
      "2014-04-03   GOOG    2.0\n",
      "Split file for GOOG corrected\n",
      "CPA\n",
      "no file found\n",
      "EXPE\n",
      "         date ticker  ratio\n",
      "0  2011-12-22   EXPE      2\n",
      "           ticker  ratio\n",
      "date                    \n",
      "2011-12-22   EXPE    1.0\n",
      "Split file for EXPE corrected\n",
      "nan\n",
      "no file found\n",
      "div file for MMM 29\n",
      "div file for AOS 29\n",
      "div file for ABT 29\n",
      "div file for ABBV 29\n",
      "div file for ACN 18\n",
      "div file for ATVI 11\n",
      "div file for ADBE 66\n",
      "div file for AAP 29\n",
      "div file for AES 30\n",
      "div file for AFL 30\n",
      "div file for A 30\n",
      "div file for APD 30\n",
      "div file for ALK 28\n",
      "div file for ALB 29\n",
      "div file for ARE 30\n",
      "div file for ALLE 28\n",
      "div file for LNT 80\n",
      "div file for ALL 29\n",
      "div file for MO 29\n",
      "div file for AMCR 5\n",
      "div file for AEE 30\n",
      "div file for AAL 23\n",
      "div file for AEP 29\n",
      "div file for AXP 29\n",
      "div file for AIG 29\n",
      "div file for AMT 30\n",
      "div file for AWK 30\n",
      "div file for AMP 29\n",
      "div file for ABC 30\n",
      "div file for AME 30\n",
      "div file for AMGN 38\n",
      "div file for APH 29\n",
      "div file for ADI 67\n",
      "div file for ANTM 24\n",
      "div file for AON 31\n",
      "div file for APA 26\n",
      "div file for AIV 31\n",
      "div file for AAPL 62\n",
      "div file for AMAT 62\n",
      "div file for APTV 10\n",
      "div file for ADM 29\n",
      "div file for AJG 29\n",
      "div file for AIZ 30\n",
      "div file for T 30\n",
      "div file for ATO 30\n",
      "div file for ADSK 63\n",
      "div file for ADP 78\n",
      "div file for AVB 30\n",
      "div file for AVY 29\n",
      "div file for BKR 5\n",
      "div file for BLL 29\n",
      "div file for BAC 29\n",
      "div file for BAX 29\n",
      "div file for BDX 29\n",
      "div file for BBY 30\n",
      "div file for BLK 30\n",
      "div file for BA 27\n",
      "div file for BWA 29\n",
      "div file for BXP 29\n",
      "div file for BMY 29\n",
      "div file for AVGO 40\n",
      "div file for BR 29\n",
      "div file for BF.B 29\n",
      "div file for CHRW 93\n",
      "div file for COG 28\n",
      "div file for CPB 30\n",
      "div file for COF 29\n",
      "div file for CAH 28\n",
      "div file for CCL 26\n",
      "div file for CARR 3\n",
      "div file for CAT 30\n",
      "div file for CBOE 36\n",
      "div file for CDW 29\n",
      "div file for CE 30\n",
      "div file for CNP 29\n",
      "div file for CTL 29\n",
      "div file for CERN 6\n",
      "div file for CF 30\n",
      "div file for SCHW 30\n",
      "div file for CVX 30\n",
      "div file for CB 31\n",
      "div file for CHD 30\n",
      "div file for CI 8\n",
      "div file for CINF 131\n",
      "div file for CTAS 33\n",
      "div file for CSCO 38\n",
      "div file for C 29\n",
      "div file for CFG 24\n",
      "div file for CTXS 8\n",
      "div file for CME 80\n",
      "div file for CMS 30\n",
      "div file for KO 29\n",
      "div file for CTSH 14\n",
      "div file for CL 30\n",
      "div file for CMCSA 92\n",
      "div file for CMA 29\n",
      "div file for CAG 29\n",
      "div file for CXO 8\n",
      "div file for COP 29\n",
      "div file for ED 30\n",
      "div file for STZ 23\n",
      "div file for GLW 29\n",
      "div file for CTVA 5\n",
      "div file for COST 71\n",
      "div file for COTY 18\n",
      "div file for CCI 27\n",
      "div file for CSX 81\n",
      "div file for CMI 29\n",
      "div file for CVS 30\n",
      "div file for DHI 29\n",
      "div file for DHR 31\n",
      "div file for DRI 26\n",
      "div file for DE 30\n",
      "div file for DAL 27\n",
      "div file for XRAY 105\n",
      "div file for DVN 31\n",
      "div file for FANG 10\n",
      "div file for DLR 29\n",
      "div file for DFS 29\n",
      "div file for DISH 4\n",
      "div file for DG 23\n",
      "div file for D 29\n",
      "div file for DPZ 29\n",
      "div file for DOV 29\n",
      "div file for DOW 24\n",
      "div file for DTE 29\n",
      "div file for DUK 30\n",
      "div file for DRE 30\n",
      "div file for DD 7\n",
      "div file for DXC 13\n",
      "div file for ETFC 7\n",
      "div file for EMN 29\n",
      "div file for ETN 31\n",
      "div file for EBAY 7\n",
      "div file for ECL 29\n",
      "div file for EIX 30\n",
      "div file for EA 1\n",
      "div file for EMR 30\n",
      "div file for ETR 29\n",
      "div file for EOG 29\n",
      "div file for EFX 30\n",
      "div file for EQIX 26\n",
      "div file for EQR 29\n",
      "div file for ESS 29\n",
      "div file for EL 28\n",
      "div file for RE 29\n",
      "div file for EVRG 10\n",
      "div file for ES 23\n",
      "div file for EXC 30\n",
      "div file for EXPE 43\n",
      "div file for EXPD 56\n",
      "div file for EXR 30\n",
      "div file for XOM 30\n",
      "div file for FAST 72\n",
      "div file for FRT 28\n",
      "div file for FDX 29\n",
      "div file for FIS 29\n",
      "div file for FITB 128\n",
      "div file for FRC 30\n",
      "div file for FE 30\n",
      "div file for FLIR 39\n",
      "div file for FLS 31\n",
      "div file for FMC 29\n",
      "div file for F 27\n",
      "div file for FTV 18\n",
      "div file for FBHS 29\n",
      "div file for FOXA 4\n",
      "div file for FOX 4\n",
      "div file for BEN 29\n",
      "div file for FCX 18\n",
      "div file for GPS 29\n",
      "div file for GRMN 46\n",
      "div file for GD 29\n",
      "div file for GE 30\n",
      "div file for GIS 30\n",
      "div file for GM 25\n",
      "div file for GPC 30\n",
      "div file for GILD 22\n",
      "div file for GPN 29\n",
      "div file for GL 5\n",
      "div file for GS 29\n",
      "div file for GWW 29\n",
      "div file for HRB 30\n",
      "div file for HAL 29\n",
      "div file for HBI 30\n",
      "div file for HIG 29\n",
      "div file for HAS 82\n",
      "div file for HCA 10\n",
      "div file for PEAK 5\n",
      "div file for HES 30\n",
      "div file for HPE 20\n",
      "div file for HLT 19\n",
      "div file for HFC 29\n",
      "div file for HD 30\n",
      "div file for HON 31\n",
      "div file for HRL 29\n",
      "div file for HST 28\n",
      "div file for HWM 2\n",
      "div file for HPQ 29\n",
      "div file for HUM 28\n",
      "div file for HBAN 127\n",
      "div file for HII 29\n",
      "div file for IEX 29\n",
      "div file for ITW 29\n",
      "div file for IR 27\n",
      "div file for INTC 113\n",
      "div file for ICE 30\n",
      "div file for IBM 29\n",
      "div file for IFF 29\n",
      "div file for IP 30\n",
      "div file for IPG 29\n",
      "div file for INTU 37\n",
      "div file for IVZ 31\n",
      "div file for IPGP 1\n",
      "div file for IRM 31\n",
      "div file for JBHT 113\n",
      "div file for JKHY 121\n",
      "div file for J 5\n",
      "div file for SJM 30\n",
      "div file for JNJ 29\n",
      "div file for JCI 29\n",
      "div file for JPM 30\n",
      "div file for JNPR 25\n",
      "div file for KSU 30\n",
      "div file for K 29\n",
      "div file for KEY 29\n",
      "div file for KMB 29\n",
      "div file for KIM 28\n",
      "div file for KMI 28\n",
      "div file for KLAC 64\n",
      "div file for KSS 27\n",
      "div file for KHC 22\n",
      "div file for KR 31\n",
      "div file for LB 26\n",
      "div file for LHX 5\n",
      "div file for LRCX 26\n",
      "div file for LW 16\n",
      "div file for LVS 27\n",
      "div file for LEG 29\n",
      "div file for LDOS 30\n",
      "div file for LEN 29\n",
      "div file for LLY 30\n",
      "div file for LNC 29\n",
      "div file for LIN 3\n",
      "div file for LMT 30\n",
      "div file for L 30\n",
      "div file for LOW 30\n",
      "div file for LYB 31\n",
      "div file for MTB 30\n",
      "div file for MRO 28\n",
      "div file for MPC 30\n",
      "div file for MKTX 45\n",
      "div file for MAR 76\n",
      "div file for MMC 30\n",
      "div file for MLM 30\n",
      "div file for MAS 30\n",
      "div file for MA 30\n",
      "div file for MXIM 71\n",
      "div file for MKC 31\n",
      "div file for MCD 29\n",
      "div file for MCK 29\n",
      "div file for MDT 30\n",
      "div file for MRK 29\n",
      "div file for MET 30\n",
      "div file for MGM 15\n",
      "div file for MCHP 72\n",
      "div file for MSFT 67\n",
      "div file for MAA 29\n",
      "div file for TAP 27\n",
      "div file for MDLZ 76\n",
      "div file for MCO 29\n",
      "div file for MS 29\n",
      "div file for MSI 29\n",
      "div file for MSCI 25\n",
      "div file for MYL 28\n",
      "div file for NDAQ 34\n",
      "div file for NOV 27\n",
      "div file for NTAP 29\n",
      "div file for NWL 81\n",
      "div file for NEM 30\n",
      "div file for NWSA 11\n",
      "div file for NWS 11\n",
      "div file for NEE 29\n",
      "div file for NLSN 29\n",
      "div file for NKE 29\n",
      "div file for NI 28\n",
      "div file for NBL 28\n",
      "div file for NSC 30\n",
      "div file for NTRS 128\n",
      "div file for NOC 30\n",
      "div file for NLOK 6\n",
      "div file for NRG 30\n",
      "div file for NUE 30\n",
      "div file for NVDA 32\n",
      "div file for OXY 29\n",
      "div file for ODFL 15\n",
      "div file for OMC 29\n",
      "div file for OKE 29\n",
      "div file for ORCL 30\n",
      "div file for OTIS 2\n",
      "div file for PCAR 161\n",
      "div file for PKG 30\n",
      "div file for PH 29\n",
      "div file for PAYX 128\n",
      "div file for PNR 31\n",
      "div file for PBCT 118\n",
      "div file for PEP 81\n",
      "div file for PKI 28\n",
      "div file for PRGO 29\n",
      "div file for PFE 30\n",
      "div file for PM 30\n",
      "div file for PSX 29\n",
      "div file for PNW 30\n",
      "div file for PXD 18\n",
      "div file for PNC 28\n",
      "div file for PPG 29\n",
      "div file for PPL 30\n",
      "div file for PFG 46\n",
      "div file for PG 30\n",
      "div file for PGR 14\n",
      "div file for PLD 30\n",
      "div file for PRU 29\n",
      "div file for PEG 29\n",
      "div file for PSA 29\n",
      "div file for PHM 30\n",
      "div file for PVH 27\n",
      "div file for QCOM 71\n",
      "div file for PWR 7\n",
      "div file for DGX 30\n",
      "div file for RL 28\n",
      "div file for RJF 29\n",
      "div file for RTX 4\n",
      "div file for O 87\n",
      "div file for REG 82\n",
      "div file for RF 29\n",
      "div file for RSG 30\n",
      "div file for RMD 30\n",
      "div file for RHI 30\n",
      "div file for ROK 30\n",
      "div file for ROL 29\n",
      "div file for ROP 30\n",
      "div file for ROST 105\n",
      "div file for RCL 28\n",
      "div file for SPGI 20\n",
      "div file for SBAC 5\n",
      "div file for SLB 29\n",
      "div file for STX 63\n",
      "div file for SEE 29\n",
      "div file for SRE 30\n",
      "div file for SHW 30\n",
      "div file for SPG 29\n",
      "div file for SWKS 26\n",
      "div file for SLG 35\n",
      "div file for SNA 29\n",
      "div file for SO 30\n",
      "div file for LUV 27\n",
      "div file for SWK 29\n",
      "div file for SBUX 43\n",
      "div file for STT 28\n",
      "div file for STE 28\n",
      "div file for SYK 29\n",
      "div file for SIVB 6\n",
      "div file for SYF 17\n",
      "div file for SYY 29\n",
      "div file for TMUS 1\n",
      "div file for TROW 129\n",
      "div file for TTWO 1\n",
      "div file for TPR 11\n",
      "div file for TGT 30\n",
      "div file for TEL 29\n",
      "div file for FTI 2\n",
      "div file for TFX 31\n",
      "div file for TXN 81\n",
      "div file for TXT 29\n",
      "div file for BK 30\n",
      "div file for CLX 29\n",
      "div file for COO 15\n",
      "div file for HSY 29\n",
      "div file for MOS 29\n",
      "div file for TRV 29\n",
      "div file for DIS 13\n",
      "div file for TMO 29\n",
      "div file for TIF 29\n",
      "div file for TJX 27\n",
      "div file for TSCO 43\n",
      "div file for TT 3\n",
      "div file for TFC 5\n",
      "div file for TSN 29\n",
      "div file for USB 30\n",
      "div file for UDR 30\n",
      "div file for ULTA 1\n",
      "div file for UNP 29\n",
      "div file for UAL 1\n",
      "div file for UNH 30\n",
      "div file for UPS 30\n",
      "div file for UHS 27\n",
      "div file for UNM 30\n",
      "div file for VLO 30\n",
      "div file for VTR 30\n",
      "div file for VRSN 2\n",
      "div file for VRSK 7\n",
      "div file for VZ 24\n",
      "div file for VFC 30\n",
      "div file for VIAC 3\n",
      "div file for V 30\n",
      "div file for VNO 30\n",
      "div file for VMC 30\n",
      "div file for WRB 29\n",
      "div file for WAB 30\n",
      "div file for WBA 81\n",
      "div file for WMT 29\n",
      "div file for WM 30\n",
      "div file for WEC 30\n",
      "div file for WFC 30\n",
      "div file for WELL 11\n",
      "div file for WST 29\n",
      "div file for WDC 30\n",
      "div file for WU 30\n",
      "div file for WRK 23\n",
      "div file for WY 28\n",
      "div file for WHR 29\n",
      "div file for WMB 30\n",
      "div file for WLTW 71\n",
      "div file for WYNN 44\n",
      "div file for XEL 81\n",
      "div file for XRX 29\n",
      "div file for XLNX 66\n",
      "div file for XYL 30\n",
      "div file for YUM 30\n",
      "div file for ZBH 21\n",
      "div file for ZION 127\n",
      "div file for ZTS 30\n",
      "A\n",
      "AAL\n",
      "AAP\n",
      "AAPL\n",
      "ABBV\n",
      "ABC\n",
      "ABMD\n",
      "ABT\n",
      "ACN\n",
      "ADBE\n",
      "ADI\n",
      "ADM\n",
      "ADP\n",
      "ADSK\n",
      "AEE\n",
      "AEP\n",
      "AES\n",
      "AFL\n",
      "AIG\n",
      "AIV\n",
      "AIZ\n",
      "AJG\n",
      "AKAM\n",
      "ALB\n",
      "ALGN\n",
      "ALK\n",
      "ALL\n",
      "ALLE\n",
      "ALXN\n",
      "AMAT\n",
      "AMCR\n",
      "AMD\n",
      "AME\n",
      "AMGN\n",
      "AMP\n",
      "AMT\n",
      "AMZN\n",
      "ANET\n",
      "ANSS\n",
      "ANTM\n",
      "AON\n",
      "AOS\n",
      "APA\n",
      "APD\n",
      "APH\n",
      "APTV\n",
      "ARE\n",
      "ATO\n",
      "ATVI\n",
      "AVB\n",
      "AVGO\n",
      "AVY\n",
      "AWK\n",
      "AXP\n",
      "AZO\n",
      "BA\n",
      "BAC\n",
      "BAX\n",
      "BBY\n",
      "BDX\n",
      "BEN\n",
      "BF.B\n",
      "BIIB\n",
      "BIO\n",
      "BK\n",
      "BKNG\n",
      "BKR\n",
      "BLK\n",
      "BLL\n",
      "BMY\n",
      "BR\n",
      "BRK.B\n",
      "BSX\n",
      "BWA\n",
      "BXP\n",
      "C\n",
      "CAG\n",
      "CAH\n",
      "CARR\n",
      "CAT\n",
      "CB\n",
      "CBOE\n",
      "CBRE\n",
      "CCI\n",
      "CCL\n",
      "CDNS\n",
      "CDW\n",
      "CE\n",
      "CERN\n",
      "CF\n",
      "CFG\n",
      "CHD\n",
      "CHRW\n",
      "CHTR\n",
      "CI\n",
      "CINF\n",
      "CL\n",
      "CLX\n",
      "CMA\n",
      "CMCSA\n",
      "CME\n",
      "CMG\n",
      "CMI\n",
      "CMS\n",
      "CNC\n",
      "CNP\n",
      "COF\n",
      "COG\n",
      "COO\n",
      "COP\n",
      "COST\n",
      "COTY\n",
      "CPB\n",
      "CPRT\n",
      "CRM\n",
      "CSCO\n",
      "CSX\n",
      "CTAS\n",
      "CTL\n",
      "CTSH\n",
      "CTVA\n",
      "CTXS\n",
      "CVS\n",
      "CVX\n",
      "CXO\n",
      "D\n",
      "DAL\n",
      "DD\n",
      "DE\n",
      "DFS\n",
      "DG\n",
      "DGX\n",
      "DHI\n",
      "DHR\n",
      "DIS\n",
      "DISCA\n",
      "DISCK\n",
      "DISH\n",
      "DLR\n",
      "DLTR\n",
      "DOV\n",
      "DOW\n",
      "DPZ\n",
      "DRE\n",
      "DRI\n",
      "DTE\n",
      "DUK\n",
      "DVA\n",
      "DVN\n",
      "DXC\n",
      "DXCM\n",
      "EA\n",
      "EBAY\n",
      "ECL\n",
      "ED\n",
      "EFX\n",
      "EIX\n",
      "EL\n",
      "EMN\n",
      "EMR\n",
      "EOG\n",
      "EQIX\n",
      "EQR\n",
      "ES\n",
      "ESS\n",
      "ETFC\n",
      "ETN\n",
      "ETR\n",
      "EVRG\n",
      "EW\n",
      "EXC\n",
      "EXPD\n",
      "EXPE\n",
      "EXR\n",
      "F\n",
      "FANG\n",
      "FAST\n",
      "FB\n",
      "FBHS\n",
      "FCX\n",
      "FDX\n",
      "FE\n",
      "FFIV\n",
      "FIS\n",
      "FISV\n",
      "FITB\n",
      "FLIR\n",
      "FLS\n",
      "FLT\n",
      "FMC\n",
      "FOX\n",
      "FOXA\n",
      "FRC\n",
      "FRT\n",
      "FTI\n",
      "FTNT\n",
      "FTV\n",
      "GD\n",
      "GE\n",
      "GILD\n",
      "GIS\n",
      "GL\n",
      "GLW\n",
      "GM\n",
      "GOOG\n",
      "GOOGL\n",
      "GPC\n",
      "GPN\n",
      "GPS\n",
      "GRMN\n",
      "GS\n",
      "GWW\n",
      "HAL\n",
      "HAS\n",
      "HBAN\n",
      "HBI\n",
      "HCA\n",
      "HD\n",
      "HES\n",
      "HFC\n",
      "HIG\n",
      "HII\n",
      "HLT\n",
      "HOLX\n",
      "HON\n",
      "HPE\n",
      "HPQ\n",
      "HRB\n",
      "HRL\n",
      "HSIC\n",
      "HST\n",
      "HSY\n",
      "HUM\n",
      "HWM\n",
      "IBM\n",
      "ICE\n",
      "IDXX\n",
      "IEX\n",
      "IFF\n",
      "ILMN\n",
      "INCY\n",
      "INFO\n",
      "INTC\n",
      "INTU\n",
      "IP\n",
      "IPG\n",
      "IPGP\n",
      "IQV\n",
      "IR\n",
      "IRM\n",
      "ISRG\n",
      "IT\n",
      "ITW\n",
      "IVZ\n",
      "J\n",
      "JBHT\n",
      "JCI\n",
      "JKHY\n",
      "JNJ\n",
      "JNPR\n",
      "JPM\n",
      "K\n",
      "KEY\n",
      "KEYS\n",
      "KHC\n",
      "KIM\n",
      "KLAC\n",
      "KMB\n",
      "KMI\n",
      "KMX\n",
      "KO\n",
      "KR\n",
      "KSS\n",
      "KSU\n",
      "L\n",
      "LB\n",
      "LDOS\n",
      "LEG\n",
      "LEN\n",
      "LH\n",
      "LHX\n",
      "LIN\n",
      "LKQ\n",
      "LLY\n",
      "LMT\n",
      "LNC\n",
      "LNT\n",
      "LOW\n",
      "LRCX\n",
      "LUV\n",
      "LVS\n",
      "LW\n",
      "LYB\n",
      "LYV\n",
      "MA\n",
      "MAA\n",
      "MAR\n",
      "MAS\n",
      "MCD\n",
      "MCHP\n",
      "MCK\n",
      "MCO\n",
      "MDLZ\n",
      "MDT\n",
      "MET\n",
      "MGM\n",
      "MHK\n",
      "MKC\n",
      "MKTX\n",
      "MLM\n",
      "MMC\n",
      "MMM\n",
      "MNST\n",
      "MO\n",
      "MOS\n",
      "MPC\n",
      "MRK\n",
      "MRO\n",
      "MS\n",
      "MSCI\n",
      "MSFT\n",
      "MSI\n",
      "MTB\n",
      "MTD\n",
      "MU\n",
      "MXIM\n",
      "MYL\n",
      "NBL\n",
      "NCLH\n",
      "NDAQ\n",
      "NEE\n",
      "NEM\n",
      "NFLX\n",
      "NI\n",
      "NKE\n",
      "NLOK\n",
      "NLSN\n",
      "NOC\n",
      "NOV\n",
      "NOW\n",
      "NRG\n",
      "NSC\n",
      "NTAP\n",
      "NTRS\n",
      "NUE\n",
      "NVDA\n",
      "NVR\n",
      "NWL\n",
      "NWS\n",
      "NWSA\n",
      "O\n",
      "ODFL\n",
      "OKE\n",
      "OMC\n",
      "ORCL\n",
      "ORLY\n",
      "OTIS\n",
      "OXY\n",
      "PAYC\n",
      "PAYX\n",
      "PBCT\n",
      "PCAR\n",
      "PEAK\n",
      "PEG\n",
      "PEP\n",
      "PFE\n",
      "PFG\n",
      "PG\n",
      "PGR\n",
      "PH\n",
      "PHM\n",
      "PKG\n",
      "PKI\n",
      "PLD\n",
      "PM\n",
      "PNC\n",
      "PNR\n",
      "PNW\n",
      "PPG\n",
      "PPL\n",
      "PRGO\n",
      "PRU\n",
      "PSA\n",
      "PSX\n",
      "PVH\n",
      "PWR\n",
      "PXD\n",
      "PYPL\n",
      "QCOM\n",
      "QRVO\n",
      "RCL\n",
      "RE\n",
      "REG\n",
      "REGN\n",
      "RF\n",
      "RHI\n",
      "RJF\n",
      "RL\n",
      "RMD\n",
      "ROK\n",
      "ROL\n",
      "ROP\n",
      "ROST\n",
      "RSG\n",
      "RTX\n",
      "SBAC\n",
      "SBUX\n",
      "SCHW\n",
      "SEE\n",
      "SHW\n",
      "SIVB\n",
      "SJM\n",
      "SLB\n",
      "SLG\n",
      "SNA\n",
      "SNPS\n",
      "SO\n",
      "SPG\n",
      "SPGI\n",
      "SRE\n",
      "STE\n",
      "STT\n",
      "STX\n",
      "STZ\n",
      "SWK\n",
      "SWKS\n",
      "SYF\n",
      "SYK\n",
      "SYY\n",
      "T\n",
      "TAP\n",
      "TDG\n",
      "TDY\n",
      "TEL\n",
      "TFC\n",
      "TFX\n",
      "TGT\n",
      "TIF\n",
      "TJX\n",
      "TMO\n",
      "TMUS\n",
      "TPR\n",
      "TROW\n",
      "TRV\n",
      "TSCO\n",
      "TSN\n",
      "TT\n",
      "TTWO\n",
      "TWTR\n",
      "TXN\n",
      "TXT\n",
      "TYL\n",
      "UA\n",
      "UAA\n",
      "UAL\n",
      "UDR\n",
      "UHS\n",
      "ULTA\n",
      "UNH\n",
      "UNM\n",
      "UNP\n",
      "UPS\n",
      "URI\n",
      "USB\n",
      "V\n",
      "VAR\n",
      "VFC\n",
      "VIAC\n",
      "VLO\n",
      "VMC\n",
      "VNO\n",
      "VRSK\n",
      "VRSN\n",
      "VRTX\n",
      "VTR\n",
      "VZ\n",
      "WAB\n",
      "WAT\n",
      "WBA\n",
      "WDC\n",
      "WEC\n",
      "WELL\n",
      "WFC\n",
      "WHR\n",
      "WLTW\n",
      "WM\n",
      "WMB\n",
      "WMT\n",
      "WRB\n",
      "WRK\n",
      "WST\n",
      "WU\n",
      "WY\n",
      "WYNN\n",
      "XEL\n",
      "XLNX\n",
      "XOM\n",
      "XRAY\n",
      "XRX\n",
      "XYL\n",
      "YUM\n",
      "ZBH\n",
      "ZBRA\n",
      "ZION\n",
      "ZTS\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'505 files was adjusted'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Fix data for about 50 splits from a correction file created manually\n",
    "fix_splits('data/splits')\n",
    "\n",
    "#%%  Pull in all the dividend data\n",
    "get_divs(symbols, 'data/divs')\n",
    "\n",
    "#%%  Combine the bars (pricing data) with any splits and dividend payments\n",
    "combine_bars('data/bars', 'data/splits', 'data/divs')\n",
    "\n",
    "#%%  Create new and stock split adjusted OHLCV fields\n",
    "adj_bars('data/bars_adj')\n",
    "\n",
    "#%%\n",
    "# bars = pd.read_csv('data/bars_adj/AAPL.csv')\n",
    "# bars['close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}